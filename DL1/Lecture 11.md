This slide introduces the concept of **benchmark sensitivity** in video self-supervised learning. Evaluating models solely on standard benchmarks, such as Kinetics-400 and UCF-101, may not accurately reflect their **generalization capabilities**. The slide, and the associated papers discuss four primary sensitivity factors:

**I. Downstream Domains:** This factor explores how well self-supervised methods, trained on a source dataset like Kinetics-400, generalize to diverse downstream datasets. The slide deck mentions several datasets including: UCF-101, NTU-60, FineGym, Something-Something-v2, EPIC-Kitchens-100, AVA, and Charades. These datasets vary considerably in their environments, viewpoints, action types, and complexity. Evaluating performance on datasets like Something-Something-v2 (SSv2) and FineGym-99, which come from domains distinct from Kinetics-400 and UCF-101, is essential for understanding the robustness of self-supervised models to domain shifts.

**II. Downstream Samples:** This factor investigates the impact of limited training samples in downstream tasks, which is especially relevant when labeled data is scarce. The slide deck emphasizes that self-supervised models are very sensitive to the amount of data available for fine-tuning. Both the performance gap between methods and their relative ranking can change significantly with varying sample sizes.

**III. Downstream Actions:** This factor assesses how well self-supervised models distinguish fine-grained and semantically similar actions. The slide deck argues that existing evaluations, often limited to coarse-grained action recognition, are insufficient. Datasets like FineGym, with its subsets designed to evaluate recognition within specific events and sets, are crucial for this type of evaluation.

**IV. Downstream Tasks:** This factor evaluates whether self-supervised models can transfer effectively to tasks beyond action recognition. The slide deck lists tasks like: spatio-temporal action detection, repetition counting, arrow-of-time prediction, and multi-label classification. The papers further extend this to tasks like unsupervised video object segmentation, and action-to-video retrieval. For example, evaluating performance on repetition counting using the UCFRep benchmark (a subset of UCF-101) helps assess a model's temporal understanding without the confounding factor of domain shift. Similarly, multi-label classification on Charades is suggested as a challenging task that combines both domain and task shifts.

The slide deck proposes the **SEVERE-benchmark** for evaluating video self-supervised methods across these four sensitivity factors. This benchmark comprises a curated subset of experiments from the study, enabling future research to assess model generalization more thoroughly. The SEVERE-benchmark evaluates models pre-trained on Kinetics-400, and proposes SS-v2 and Gym-99 as the downstream domains, UCF-101 and Gym-99 with 1000 samples each as the downstream samples, FX-S1 and UB-S1 from Gym-99 to test the downstream actions, and UCF-RC and Charades-MLC as the downstream tasks. The SEVERE benchmark is a valuable tool for evaluating the generalization capabilities of new video self-supervised methods and provides a more comprehensive picture than traditional benchmarks.

Beyond the SEVERE benchmark, both the slide deck and the papers emphasize additional factors and concepts related to generalization in video self-supervised learning.

- **Temporal Awareness:** This refers to a model's ability to comprehend temporal relationships between events in a video. This ability is crucial for tasks like action recognition, action-to-video retrieval, and question answering. Datasets like SSv2, specifically designed to capture rich temporal information, effectively assess models' temporal modeling capabilities. Additionally, tasks like repetition counting and evaluating temporal order consistency also provide valuable insights into a model's temporal understanding.
    
- **Motion-focused Representations**: Learning representations that prioritize motion dynamics over appearance is essential for robust action recognition. This can be achieved by explicitly focusing on local motion dynamics, for instance, by incorporating synthetic motion trajectories, referred to as tubelets. Tubelet-contrastive learning encourages models to differentiate actions based on motion patterns rather than solely relying on spatial features, improving their data efficiency and generalization capabilities, particularly for fine-grained action recognition.
    
- **Reconstruction Target Space:** Moving beyond predefined low-level targets like pixels in masked video modeling and towards learning a more abstract and semantic target space can significantly improve performance. The slide deck advocates for a learnable target space that is optimized alongside the video model. Employing techniques like Sinkhorn-clustering to partition the feature space into clusters enhances the semantic richness of the learned representations while mitigating the issue of training collapse. By incorporating pretrained image models, such as DINO, as targets, the learned video models benefit from improved spatial and temporal semantics and demonstrate greater robustness.